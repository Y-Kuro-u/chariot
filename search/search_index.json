{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"chariot Deliver the ready-to-train data to your NLP model. Introduction Prepare Dataset You can prepare typical NLP datasets through the chazutsu . Build & Run Preprocess You can build the preprocess pipeline like scikit-learn Pipeline . Preprocesses for each dataset column are executed in parallel by Joblib . Multi-language text tokenization is supported by spaCy . Format Batch Sampling a batch from preprocessed dataset and format it to train the model (padding etc). You can use pre-trained word vectors through the chakin chariot enables you to concentrate on training your model! License Apache License 2.0","title":"Home"},{"location":"#introduction","text":"Prepare Dataset You can prepare typical NLP datasets through the chazutsu . Build & Run Preprocess You can build the preprocess pipeline like scikit-learn Pipeline . Preprocesses for each dataset column are executed in parallel by Joblib . Multi-language text tokenization is supported by spaCy . Format Batch Sampling a batch from preprocessed dataset and format it to train the model (padding etc). You can use pre-trained word vectors through the chakin chariot enables you to concentrate on training your model!","title":"Introduction"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"tutorial/install/","text":"Install Install by pip You can install the chariot by pip command. pip install chariot That'all! Next, let's preprocess the text .","title":"Installation"},{"location":"tutorial/install/#install-by-pip","text":"You can install the chariot by pip command. pip install chariot That'all! Next, let's preprocess the text .","title":"Install by pip"},{"location":"tutorial/make_custom_preprocessor/","text":"Make custom preprocessors If prepared preprocessors are not enough to you, you can make custom preprocessors. Base classes for customizing Following base classes are prepared to make custom preprocessor. Text reprocessor: TextNormalizer and TextFilter Tokenizer: - Token preprocessor: TokenNormalizer and TokenFilter Vocabulary: - Formatter: BaseFormatter Generator: SourceGenerator and TargetGenerator All you have to do is implements apply or transform method for each base class.","title":"Make custom preprocessors"},{"location":"tutorial/make_custom_preprocessor/#base-classes-for-customizing","text":"Following base classes are prepared to make custom preprocessor. Text reprocessor: TextNormalizer and TextFilter Tokenizer: - Token preprocessor: TokenNormalizer and TokenFilter Vocabulary: - Formatter: BaseFormatter Generator: SourceGenerator and TargetGenerator All you have to do is implements apply or transform method for each base class.","title":"Base classes for customizing"},{"location":"tutorial/make_preprocessor/","text":"Make preprocessor There are 6 kinds of preprocessor in chariot . Text reprocessor Tokenizer Token preprocessor Vocabulary Formatter Generator Each preprocessor is defined as scikit-learn Transformer . Because of this, these preprocessors locate at chariot.transformer . You can initialize parameters of preprocessor by fit , and apply preprocess by transform . Text preprocessor The role of the Text preprocessor is arranging text before tokenization. import chariot.transformer as ct text = \"Hey! you preprocess text now :)\" preprocessed = ct.text.SymbolFilter().transform(text) > Hey you preprocess text now Tokenizer The role of the Tokenizer is tokenizing text. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) > [<Hey:INTJ>, <you:PRON>, <preprocess:ADJ>, <text:NOUN>, <now:ADV>] When tokenize a text, chariot use spacy mainly. You can specify language by lang parameter. But if you want to tokenize Japanese text, you have to prepare the Janome or MeCab since spacy does not support Japanese well. Token preprocessor The role of the Token preprocessor is filter/normalize tokens before building vocabulary. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) filtered = ct.token.StopwordFilter(lang=\"en\").transform(tokens) > [<Hey:INTJ>, <preprocess:ADJ>, <text:NOUN>] Vocabulary The role of the Vocabulary is convert word to vocabulary index. import chariot.transformer as ct vocab = Vocabulary() doc = [ [\"you\", \"are\", \"reading\", \"the\", \"book\"], [\"I\", \"don't\", \"know\", \"its\", \"title\"], ] vocab.fit(doc) text = [\"you\", \"know\", \"book\", \"title\"] indexed = vocab.transform(text) inversed = vocab.inverse_transform(indexed) > [4, 11, 8, 13] > ['you', 'know', 'book', 'title'] You can specify the reserved word for unknown word etc and set parameters to limit vocabulary size. Example like following. vocab = Vocabulary(padding=\"_pad_\", unknown=\"_unk_\", min_df=1) Formatter The role of the Formatter is adjust data for your model. import chariot.transformer as ct formatter = ct.formatter.Padding(padding=0, length=5) data = [ [1, 2], [3, 4, 5], [1, 2, 3, 4, 5] ] padded = formatter.transform(data) > [[1 2 0 0 0] [3 4 5 0 0] [1 2 3 4 5]] Generator The role of the Generator is generating the target / source data for your model. For example, when you train the language model, your target data is shifted source data. import chariot.transformer as ct generator = ct.generator.ShiftedTarget(shift=1) source, target = generator.generate([1, 2, 3, 4, 5], index=0, length=3) > source [1, 2, 3] > target [2, 3, 4] Now you learned the role of each preprocessor. Then let's make preprocessor pipeline by composing these .","title":"Make preprocessor"},{"location":"tutorial/make_preprocessor/#text-preprocessor","text":"The role of the Text preprocessor is arranging text before tokenization. import chariot.transformer as ct text = \"Hey! you preprocess text now :)\" preprocessed = ct.text.SymbolFilter().transform(text) > Hey you preprocess text now","title":"Text preprocessor"},{"location":"tutorial/make_preprocessor/#tokenizer","text":"The role of the Tokenizer is tokenizing text. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) > [<Hey:INTJ>, <you:PRON>, <preprocess:ADJ>, <text:NOUN>, <now:ADV>] When tokenize a text, chariot use spacy mainly. You can specify language by lang parameter. But if you want to tokenize Japanese text, you have to prepare the Janome or MeCab since spacy does not support Japanese well.","title":"Tokenizer"},{"location":"tutorial/make_preprocessor/#token-preprocessor","text":"The role of the Token preprocessor is filter/normalize tokens before building vocabulary. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) filtered = ct.token.StopwordFilter(lang=\"en\").transform(tokens) > [<Hey:INTJ>, <preprocess:ADJ>, <text:NOUN>]","title":"Token preprocessor"},{"location":"tutorial/make_preprocessor/#vocabulary","text":"The role of the Vocabulary is convert word to vocabulary index. import chariot.transformer as ct vocab = Vocabulary() doc = [ [\"you\", \"are\", \"reading\", \"the\", \"book\"], [\"I\", \"don't\", \"know\", \"its\", \"title\"], ] vocab.fit(doc) text = [\"you\", \"know\", \"book\", \"title\"] indexed = vocab.transform(text) inversed = vocab.inverse_transform(indexed) > [4, 11, 8, 13] > ['you', 'know', 'book', 'title'] You can specify the reserved word for unknown word etc and set parameters to limit vocabulary size. Example like following. vocab = Vocabulary(padding=\"_pad_\", unknown=\"_unk_\", min_df=1)","title":"Vocabulary"},{"location":"tutorial/make_preprocessor/#formatter","text":"The role of the Formatter is adjust data for your model. import chariot.transformer as ct formatter = ct.formatter.Padding(padding=0, length=5) data = [ [1, 2], [3, 4, 5], [1, 2, 3, 4, 5] ] padded = formatter.transform(data) > [[1 2 0 0 0] [3 4 5 0 0] [1 2 3 4 5]]","title":"Formatter"},{"location":"tutorial/make_preprocessor/#generator","text":"The role of the Generator is generating the target / source data for your model. For example, when you train the language model, your target data is shifted source data. import chariot.transformer as ct generator = ct.generator.ShiftedTarget(shift=1) source, target = generator.generate([1, 2, 3, 4, 5], index=0, length=3) > source [1, 2, 3] > target [2, 3, 4] Now you learned the role of each preprocessor. Then let's make preprocessor pipeline by composing these .","title":"Generator"},{"location":"tutorial/make_preprocessor_pipeline/","text":"Make preprocessor pipeline You can combine each preprocessor to make pipeline process. As the name pipeline indicates, it just same as the scikit-learn Pipeline . Define a Pipeline You can use Preprocessor to combine each preprocessors. import chariot.transformer as ct from chariot.preprocessor import Preprocessor preprocessor = Preprocessor() preprocessor\\ .stack(ct.text.UnicodeNormalizer())\\ .stack(ct.Tokenizer(\"en\"))\\ .stack(ct.token.StopwordFilter(\"en\"))\\ .stack(ct.Vocabulary(min_df=5, max_df=0.5))\\ .fit(train_data) preprocessed = preprocessor.transform(data) You can save & load the Preprocessor . preprocessor.save(\"my_preprocessor.pkl\") loaded = Preprocessor.load(\"my_preprocessor.pkl\") It means you can pack & carry the preprocess by .pkl file. Make pipeline for dataset When you want to apply distinctive preprocess for each column of a dataset, you can use DatasetPreprocessor . from chariot.dataset_preprocessor import DatasetPreprocessor from chariot.transformer.formatter import Padding dp = DatasetPreprocessor() dp.process(\"review\")\\ .by(ct.text.UnicodeNormalizer())\\ .by(ct.Tokenizer(\"en\"))\\ .by(ct.token.StopwordFilter(\"en\"))\\ .by(ct.Vocabulary(min_df=5, max_df=0.5))\\ .by(Padding(length=pad_length))\\ .fit(train_data[\"review\"]) dp.process(\"polarity\")\\ .by(ct.formatter.CategoricalLabel(num_class=3)) preprocessed = dp.preprocess(data) You can save & load DatasetPreprocessor as preprocessor. dp.save(\"my_dataset_preprocessor.tar.gz\") loaded = DatasetPreprocessor.load(\"my_dataset_preprocessor.tar.gz\") Why you preprocess the data? Of course you want to train your model! Next feed data to your model .","title":"Make preprocessor pipeline"},{"location":"tutorial/make_preprocessor_pipeline/#define-a-pipeline","text":"You can use Preprocessor to combine each preprocessors. import chariot.transformer as ct from chariot.preprocessor import Preprocessor preprocessor = Preprocessor() preprocessor\\ .stack(ct.text.UnicodeNormalizer())\\ .stack(ct.Tokenizer(\"en\"))\\ .stack(ct.token.StopwordFilter(\"en\"))\\ .stack(ct.Vocabulary(min_df=5, max_df=0.5))\\ .fit(train_data) preprocessed = preprocessor.transform(data) You can save & load the Preprocessor . preprocessor.save(\"my_preprocessor.pkl\") loaded = Preprocessor.load(\"my_preprocessor.pkl\") It means you can pack & carry the preprocess by .pkl file.","title":"Define a Pipeline"},{"location":"tutorial/make_preprocessor_pipeline/#make-pipeline-for-dataset","text":"When you want to apply distinctive preprocess for each column of a dataset, you can use DatasetPreprocessor . from chariot.dataset_preprocessor import DatasetPreprocessor from chariot.transformer.formatter import Padding dp = DatasetPreprocessor() dp.process(\"review\")\\ .by(ct.text.UnicodeNormalizer())\\ .by(ct.Tokenizer(\"en\"))\\ .by(ct.token.StopwordFilter(\"en\"))\\ .by(ct.Vocabulary(min_df=5, max_df=0.5))\\ .by(Padding(length=pad_length))\\ .fit(train_data[\"review\"]) dp.process(\"polarity\")\\ .by(ct.formatter.CategoricalLabel(num_class=3)) preprocessed = dp.preprocess(data) You can save & load DatasetPreprocessor as preprocessor. dp.save(\"my_dataset_preprocessor.tar.gz\") loaded = DatasetPreprocessor.load(\"my_dataset_preprocessor.tar.gz\") Why you preprocess the data? Of course you want to train your model! Next feed data to your model .","title":"Make pipeline for dataset"},{"location":"tutorial/prepare_resources/","text":"Prepare Resources chariot have the feature to prepare resources for NLP research. Specifically, data and pretrained vectors. Download NLP dataset chariot can collaborate with chazutsu that is NLP datasets downloader. import chazutsu from chariot.storage import Storage storage = Storage.setup_data_dir(ROOT_DIR) r = chazutsu.datasets.MovieReview.polarity().download(storage.data_path(\"raw\")) r.train_data().head(3) polarity review 0 0 synopsis : an aging master art thief , his sup... 1 0 plot : a separated , glamorous , hollywood cou... 2 0 a friend invites you to a movie . this film wo... Download Pretrained Word Vector chariot can load the pretrained word vector by collaborating with chakin . storage = Storage(\"path/to/project/root\") vec_path = storage.chakin(name=\"GloVe.6B.200d\") # download word vector vocab = Vocabulary.from_file(\"path/to/vocabulary\") embedding = vocab.make_embedding(storage.data_path(\"external/glove.6B.200d.txt\"))","title":"Prepare NLP resources"},{"location":"tutorial/prepare_resources/#download-nlp-dataset","text":"chariot can collaborate with chazutsu that is NLP datasets downloader. import chazutsu from chariot.storage import Storage storage = Storage.setup_data_dir(ROOT_DIR) r = chazutsu.datasets.MovieReview.polarity().download(storage.data_path(\"raw\")) r.train_data().head(3) polarity review 0 0 synopsis : an aging master art thief , his sup... 1 0 plot : a separated , glamorous , hollywood cou... 2 0 a friend invites you to a movie . this film wo...","title":"Download NLP dataset"},{"location":"tutorial/prepare_resources/#download-pretrained-word-vector","text":"chariot can load the pretrained word vector by collaborating with chakin . storage = Storage(\"path/to/project/root\") vec_path = storage.chakin(name=\"GloVe.6B.200d\") # download word vector vocab = Vocabulary.from_file(\"path/to/vocabulary\") embedding = vocab.make_embedding(storage.data_path(\"external/glove.6B.200d.txt\"))","title":"Download Pretrained Word Vector"},{"location":"tutorial/train_model/","text":"Train your model You can easily feed the preprocessed data to your model by chariot . for batch in dp(train_data.preprocess().iterate(batch_size=32, epoch=10): model.train_on_batch(batch[\"review\"], batch[\"polarity\"]) If you want to preprocess/format all the data before training, you can do it like following. formatted = dp(train_data).preprocess().format().processed model.fit(formatted[\"review\"], formatted[\"polarity\"], batch_size=32, validation_split=0.2, epochs=15, verbose=2) Congratulations! You are ready to collaborate with chariot . Next is additional content to introduce other convenient features of chariot .","title":"Train your model"}]}