{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"chariot Deliver the ready-to-train data to your NLP model. Introduction Prepare Dataset You can prepare typical NLP datasets through the chazutsu . Build & Run Preprocess You can build the preprocess pipeline like scikit-learn Pipeline . Preprocesses for each dataset column are executed in parallel by Joblib . Multi-language text tokenization is supported by spaCy . Format Batch Sampling a batch from preprocessed dataset and format it to train the model (padding etc). You can use pre-trained word vectors through the chakin chariot enables you to concentrate on training your model! License Apache License 2.0","title":"Home"},{"location":"#introduction","text":"Prepare Dataset You can prepare typical NLP datasets through the chazutsu . Build & Run Preprocess You can build the preprocess pipeline like scikit-learn Pipeline . Preprocesses for each dataset column are executed in parallel by Joblib . Multi-language text tokenization is supported by spaCy . Format Batch Sampling a batch from preprocessed dataset and format it to train the model (padding etc). You can use pre-trained word vectors through the chakin chariot enables you to concentrate on training your model!","title":"Introduction"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"tutorial/install/","text":"Install Install by pip You can install the chariot by pip command. pip install chariot That'all! Next, let's preprocess the text .","title":"Installation"},{"location":"tutorial/install/#install-by-pip","text":"You can install the chariot by pip command. pip install chariot That'all! Next, let's preprocess the text .","title":"Install by pip"},{"location":"tutorial/make_custom_preprocessor/","text":"Make custom preprocessors If prepared preprocessors are not enough to you, you can make custom preprocessors. Base classes for customizing Following base classes are prepared to make custom preprocessor. Text reprocessor: TextNormalizer and TextFilter Tokenizer: - Token preprocessor: TokenNormalizer and TokenFilter Vocabulary: - Formatter: BaseFormatter All you have to do is implements apply or transform method for each base class.","title":"Make custom preprocessors"},{"location":"tutorial/make_custom_preprocessor/#base-classes-for-customizing","text":"Following base classes are prepared to make custom preprocessor. Text reprocessor: TextNormalizer and TextFilter Tokenizer: - Token preprocessor: TokenNormalizer and TokenFilter Vocabulary: - Formatter: BaseFormatter All you have to do is implements apply or transform method for each base class.","title":"Base classes for customizing"},{"location":"tutorial/make_pipeline/","text":"Make a preprocessor pipeline You can stack each preprocessor as preprocessor pipeline. As the name pipeline indicates, it just same as the scikit-learn Pipeline . Define a Pipeline You can use Preprocessor to combine each preprocessors. import chariot.transformer as ct from chariot.preprocessor import Preprocessor preprocessor = Preprocessor( tokenizer=ct.Tokenizer(\"en\"), text_transformers=[ct.text.UnicodeNormalizer()], token_transformers=[ct.token.StopwordFilter(\"en\")], vocabulary=ct.Vocabulary(min_df=3, max_df=1.0)) preprocessed = preprocessor.fit_transform(text_document) Save and load a pipeline You can save & load the Preprocessor . preprocessor.save(\"/path/to/preprocessor_name.pkl\") preprocessor = Preprocess.load(\"/path/to/preprocessor_name.pkl\") It means you can deploy a serialized preprocessor and run it without its definition code! Apply individual pipeline If you have multiple text column, you'll want to change preprocess column by column. Preprocess enables it. preprocess = Preprocess({ \"question\": question_preprocessor, \"document\": document_preprocessor }) Or apply multiple preprocessors to one column. preprocess = Preprocess({ \"document\": { \"word\": word_preprocessor, \"charactor\": char_preprocessor } }) You can save & load Preprocess as preprocessor. preprocess.save(\"/path/to/preprocess.tar.gz\") preprocess = Preprocess.load(\"/path/to/preprocess.tar.gz\" Now you prepare the data for your model. Then feed it to your model !","title":"Make a preprocessor pipeline"},{"location":"tutorial/make_pipeline/#define-a-pipeline","text":"You can use Preprocessor to combine each preprocessors. import chariot.transformer as ct from chariot.preprocessor import Preprocessor preprocessor = Preprocessor( tokenizer=ct.Tokenizer(\"en\"), text_transformers=[ct.text.UnicodeNormalizer()], token_transformers=[ct.token.StopwordFilter(\"en\")], vocabulary=ct.Vocabulary(min_df=3, max_df=1.0)) preprocessed = preprocessor.fit_transform(text_document)","title":"Define a Pipeline"},{"location":"tutorial/make_pipeline/#save-and-load-a-pipeline","text":"You can save & load the Preprocessor . preprocessor.save(\"/path/to/preprocessor_name.pkl\") preprocessor = Preprocess.load(\"/path/to/preprocessor_name.pkl\") It means you can deploy a serialized preprocessor and run it without its definition code!","title":"Save and load a pipeline"},{"location":"tutorial/make_pipeline/#apply-individual-pipeline","text":"If you have multiple text column, you'll want to change preprocess column by column. Preprocess enables it. preprocess = Preprocess({ \"question\": question_preprocessor, \"document\": document_preprocessor }) Or apply multiple preprocessors to one column. preprocess = Preprocess({ \"document\": { \"word\": word_preprocessor, \"charactor\": char_preprocessor } }) You can save & load Preprocess as preprocessor. preprocess.save(\"/path/to/preprocess.tar.gz\") preprocess = Preprocess.load(\"/path/to/preprocess.tar.gz\" Now you prepare the data for your model. Then feed it to your model !","title":"Apply individual pipeline"},{"location":"tutorial/make_preprocessor/","text":"Make preprocessors There are 5 kinds of preprocessor in chariot . Text reprocessor Tokenizer Token preprocessor Vocabulary Formatter Each preprocessor is defined as scikit-learn Transformer . Because of this, these preprocessors locate at chariot.transformer . You can initialize parameters of preprocessor by fit , and apply preprocess by transform . Text preprocessor The role of the Text preprocessor is arranging text before tokenization. import chariot.transformer as ct text = \"Hey! you preprocess text now :)\" preprocessed = ct.text.SymbolFilter().transform(text) > Hey you preprocess text now Tokenizer The role of the Tokenizer is tokenizing text. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) > [<Hey:INTJ>, <you:PRON>, <preprocess:ADJ>, <text:NOUN>, <now:ADV>] When tokenize a text, chariot use spacy mainly. You can specify language by lang parameter. But if you want to tokenize Japanese text, you have to prepare the Janome or MeCab since spacy does not support Japanese well. Token preprocessor The role of the Token preprocessor is filter/normalize tokens before building vocabulary. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) filtered = ct.token.StopwordFilter(lang=\"en\").transform(tokens) > [<Hey:INTJ>, <preprocess:ADJ>, <text:NOUN>] Vocabulary The role of the Vocabulary is convert word to vocabulary index. import chariot.transformer as ct vocab = Vocabulary() doc = [ [\"you\", \"are\", \"reading\", \"the\", \"book\"], [\"I\", \"don't\", \"know\", \"its\", \"title\"], ] vocab.fit(doc) text = [\"you\", \"know\", \"book\", \"title\"] indexed = vocab.transform(text) inversed = vocab.inverse_transform(indexed) > [4, 11, 8, 13] > ['you', 'know', 'book', 'title'] You can specify the reserved word for unknown word etc and set parameters to limit vocabulary size. Example like following. vocab = Vocabulary(padding=\"_pad_\", unknown=\"_unk_\", min_df=1) Formatter The role of the Formatter is adjust data for your model. import chariot.transformer as ct formatter = ct.formatter.Padding(padding=0, length=5) data = [ [1, 2], [3, 4, 5], [1, 2, 3, 4, 5] ] padded = formatter.transform(data) > [[1 2 0 0 0] [3 4 5 0 0] [1 2 3 4 5]] Of Course you can use Formatter to batch data. Now you learned the role of each preprocessor. Then let's make preprocessor pipeline by composing these .","title":"Make preprocessors"},{"location":"tutorial/make_preprocessor/#text-preprocessor","text":"The role of the Text preprocessor is arranging text before tokenization. import chariot.transformer as ct text = \"Hey! you preprocess text now :)\" preprocessed = ct.text.SymbolFilter().transform(text) > Hey you preprocess text now","title":"Text preprocessor"},{"location":"tutorial/make_preprocessor/#tokenizer","text":"The role of the Tokenizer is tokenizing text. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) > [<Hey:INTJ>, <you:PRON>, <preprocess:ADJ>, <text:NOUN>, <now:ADV>] When tokenize a text, chariot use spacy mainly. You can specify language by lang parameter. But if you want to tokenize Japanese text, you have to prepare the Janome or MeCab since spacy does not support Japanese well.","title":"Tokenizer"},{"location":"tutorial/make_preprocessor/#token-preprocessor","text":"The role of the Token preprocessor is filter/normalize tokens before building vocabulary. import chariot.transformer as ct text = \"Hey you preprocess text now\" tokens = ct.Tokenizer(lang=\"en\").transform(text) filtered = ct.token.StopwordFilter(lang=\"en\").transform(tokens) > [<Hey:INTJ>, <preprocess:ADJ>, <text:NOUN>]","title":"Token preprocessor"},{"location":"tutorial/make_preprocessor/#vocabulary","text":"The role of the Vocabulary is convert word to vocabulary index. import chariot.transformer as ct vocab = Vocabulary() doc = [ [\"you\", \"are\", \"reading\", \"the\", \"book\"], [\"I\", \"don't\", \"know\", \"its\", \"title\"], ] vocab.fit(doc) text = [\"you\", \"know\", \"book\", \"title\"] indexed = vocab.transform(text) inversed = vocab.inverse_transform(indexed) > [4, 11, 8, 13] > ['you', 'know', 'book', 'title'] You can specify the reserved word for unknown word etc and set parameters to limit vocabulary size. Example like following. vocab = Vocabulary(padding=\"_pad_\", unknown=\"_unk_\", min_df=1)","title":"Vocabulary"},{"location":"tutorial/make_preprocessor/#formatter","text":"The role of the Formatter is adjust data for your model. import chariot.transformer as ct formatter = ct.formatter.Padding(padding=0, length=5) data = [ [1, 2], [3, 4, 5], [1, 2, 3, 4, 5] ] padded = formatter.transform(data) > [[1 2 0 0 0] [3 4 5 0 0] [1 2 3 4 5]] Of Course you can use Formatter to batch data. Now you learned the role of each preprocessor. Then let's make preprocessor pipeline by composing these .","title":"Formatter"},{"location":"tutorial/model_integration/","text":"Model Integration You have to arrange the data format to feed it to your model. It's known as padding, one-hot vectorize etc. Of course, chariot supports these feature. Make Data feeding process Feeder enables it. import chariot.transformer as ct preprocessed_data = { \"label\": [...], \"review\": [...] } feeder = Feeder({\"label\": ct.formatter.CategoricalLabel(num_class=5), \"review\": ct.formatter.Padding.from_(padding=0, length=5)}) adjusted = feeder.transform(preprocessed_data) Feeder supports batch iteration. for batch in feeder.iterate(preprocessed_data, batch_size=32, epoch=10): model.train_on_batch(batch[\"review\"], batch[\"label\"]) If you have preprocessor , you can use it to define each formatter s. import chariot.transformer as ct feeder = Feeder({\"label\": ct.formatter.CategoricalLabel.from_(preprocessor), \"review\": ct.formatter.Padding.from_(preprocessor, length=5)}) Save and load a feeder Feeder can bed saved to file like preprocess . feeder.save(\"/path/to/feeder.tar.gz\") feeder = Feeder.load(\"/path/to/feeder.tar.gz\") Congratulations! You are ready to collaborate with chariot . Next is additional content to introduce other convenient features of chariot .","title":"Work with your Model"},{"location":"tutorial/model_integration/#make-data-feeding-process","text":"Feeder enables it. import chariot.transformer as ct preprocessed_data = { \"label\": [...], \"review\": [...] } feeder = Feeder({\"label\": ct.formatter.CategoricalLabel(num_class=5), \"review\": ct.formatter.Padding.from_(padding=0, length=5)}) adjusted = feeder.transform(preprocessed_data) Feeder supports batch iteration. for batch in feeder.iterate(preprocessed_data, batch_size=32, epoch=10): model.train_on_batch(batch[\"review\"], batch[\"label\"]) If you have preprocessor , you can use it to define each formatter s. import chariot.transformer as ct feeder = Feeder({\"label\": ct.formatter.CategoricalLabel.from_(preprocessor), \"review\": ct.formatter.Padding.from_(preprocessor, length=5)})","title":"Make Data feeding process"},{"location":"tutorial/model_integration/#save-and-load-a-feeder","text":"Feeder can bed saved to file like preprocess . feeder.save(\"/path/to/feeder.tar.gz\") feeder = Feeder.load(\"/path/to/feeder.tar.gz\") Congratulations! You are ready to collaborate with chariot . Next is additional content to introduce other convenient features of chariot .","title":"Save and load a feeder"},{"location":"tutorial/prepare_resources/","text":"Prepare Resources chariot have the feature to prepare resources for NLP research. Specifically, data and pretrained vectors. Download NLP dataset chariot can collaborate with chazutsu that is NLP datasets downloader. import chazutsu from chariot.storage import Storage storage = Storage.setup_data_dir(ROOT_DIR) r = chazutsu.datasets.MovieReview.polarity().download(storage.data_path(\"raw\")) r.train_data().head(3) polarity review 0 0 synopsis : an aging master art thief , his sup... 1 0 plot : a separated , glamorous , hollywood cou... 2 0 a friend invites you to a movie . this film wo... Download Pretrained Word Vector chariot can load the pretrained word vector by collaborating with chakin . storage = Storage(\"path/to/project/root\") vec_path = storage.chakin(name=\"GloVe.6B.200d\") # download word vector vocab = Vocabulary.from_file(\"path/to/vocabulary\") embedding = vocab.make_embedding(storage.data_path(\"external/glove.6B.200d.txt\"))","title":"Prepare NLP resources"},{"location":"tutorial/prepare_resources/#download-nlp-dataset","text":"chariot can collaborate with chazutsu that is NLP datasets downloader. import chazutsu from chariot.storage import Storage storage = Storage.setup_data_dir(ROOT_DIR) r = chazutsu.datasets.MovieReview.polarity().download(storage.data_path(\"raw\")) r.train_data().head(3) polarity review 0 0 synopsis : an aging master art thief , his sup... 1 0 plot : a separated , glamorous , hollywood cou... 2 0 a friend invites you to a movie . this film wo...","title":"Download NLP dataset"},{"location":"tutorial/prepare_resources/#download-pretrained-word-vector","text":"chariot can load the pretrained word vector by collaborating with chakin . storage = Storage(\"path/to/project/root\") vec_path = storage.chakin(name=\"GloVe.6B.200d\") # download word vector vocab = Vocabulary.from_file(\"path/to/vocabulary\") embedding = vocab.make_embedding(storage.data_path(\"external/glove.6B.200d.txt\"))","title":"Download Pretrained Word Vector"}]}